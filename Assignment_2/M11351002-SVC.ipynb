{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Support Vector Machines - SVC\n",
    "\n",
    "Hi all, \n",
    "\n",
    "Please use google to find out SVM python code and then use it to further produce prediction results (regression and classification). \n",
    "\n",
    "With warm regards,\n",
    "\n",
    "Stanley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量分類（SVC）\n",
    "\n",
    "SVC是一種用於分類任務的支持向量機。其目的是找到一個最佳的超平面，將不同類別的數據點分開。SVC的主要目標是最大化分類邊界兩側最近數據點之間的間隔，以提高模型的泛化能力。\n",
    "\n",
    "### 主要特點：\n",
    "- **分類任務**：SVC適用於二元或多元分類問題。\n",
    "- **超平面**：在高維空間中找到一個最佳的超平面來分隔不同類別的數據。\n",
    "- **支持向量**：決定最佳超平面位置的數據點。\n",
    "- **核函數**：可以使用不同的核函數（如線性核、多項式核、RBF核）來處理線性和非線性可分的數據。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanley Recommends\n",
    "\n",
    "用鐵達尼號的那個資料集，並且要做前處理，one-hot encoding，填補缺失值等等。\n",
    "\n",
    "[Taitanic Dataset](https://www.kaggle.com/c/titanic/data)\n",
    "\n",
    "### 參考資料：\n",
    "[Titanic - Machine Learning from Disaster 鐵達尼號生存預測 資料分析篇](https://hackmd.io/@Go3PyC86QhypSl7kh5nA2Q/Hk4nXFYkK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 讓你的圖形直接嵌入到 Notebook 中，而不是另開視窗。\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "\n",
    "### 欄位解釋\n",
    "\n",
    "| Variable  | Definition                           | Key                               |\n",
    "|-----------|--------------------------------------|-----------------------------------|\n",
    "| survival  | Survival                             | 0 = No, 1 = Yes                   |\n",
    "| pclass    | Ticket class                         | 1 = 1st, 2 = 2nd, 3 = 3rd         |\n",
    "| sex       | Sex                                  |                                   |\n",
    "| Age       | Age in years                         |                                   |\n",
    "| sibsp     | # of siblings / spouses aboard the Titanic |                           |\n",
    "| parch     | # of parents / children aboard the Titanic |                           |\n",
    "| ticket    | Ticket number                        |                                   |\n",
    "| fare      | Passenger fare                       |                                   |\n",
    "| cabin     | Cabin number                         |                                   |\n",
    "| embarked  | Port of Embarkation                  | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/hank/CodeSpace/1131-ML/Assignment_2/dataset/Taitanic/train.csv')\n",
    "test_df = pd.read_csv('/Users/hank/CodeSpace/1131-ML/Assignment_2/dataset/Taitanic/test.csv') #無Survived欄位\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "1. `Cabin`缺失值過多，對訓練無幫助，直接移除\n",
    "2. `PassengerId`、`Name`、`Ticket`(票號)，沒有分析價值，直接移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢視缺失值\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# train_df 資料總筆數\n",
    "print(\"資料總筆數：\" + str(train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除欄位\n",
    "train_df = train_df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# 移除有缺失值的資料\n",
    "train_df = train_df.dropna(subset =  ['Embarked','Age'])\n",
    "\n",
    "# 檢視\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "\n",
    "`Sex` 與 `Embarked` 是非數值型資料，需要進行編碼。\n",
    "\n",
    "使用LabelEncoder。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values in the columns\n",
    "print(train_df['Sex'].unique())\n",
    "print(train_df['Embarked'].unique())\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# Encode the sex column\n",
    "train_df.iloc[:, 2] = labelencoder.fit_transform(train_df.iloc[:, 2].values)\n",
    "\n",
    "# Encode the embarked column\n",
    "train_df.iloc[:, 7] = labelencoder.fit_transform(train_df.iloc[:, 7].values)\n",
    "\n",
    "# Print the unique values in the columns\n",
    "print(train_df['Sex'].unique())\n",
    "print(train_df['Embarked'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into independent 'X' and dependent 'y' variables\n",
    "X = train_df.iloc[:, 1:8].values\n",
    "y = train_df.iloc[:, 0].values\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_trian = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "def models(X_train, y_train):\n",
    "    # Use SVC (linear kernal)\n",
    "    svc_lin = SVC(kernel='linear', random_state = 0)\n",
    "    svc_lin.fit(X_train,y_train)\n",
    "\n",
    "    # Use SVC (RBF kernal)\n",
    "    svc_rbf = SVC(kernel='rbf', random_state = 0)\n",
    "    svc_rbf.fit(X_train,y_train)\n",
    "\n",
    "    print('SVC Linear Training Accuracy:', svc_lin.score(X_train, y_train))\n",
    "    print('SVC RBF Training Accuracy:', svc_rbf.score(X_train, y_train))\n",
    "\n",
    "    # Save the model\n",
    "    # pickle.dump(svc_lin, open('/Users/hank/CodeSpace/1131-ML/Assignment_2/svc_lin_model.pkl', 'wb'))\n",
    "    # pickle.dump(svc_rbf, open('/Users/hank/CodeSpace/1131-ML/Assignment_2/svc_rbf_model.pkl', 'wb'))\n",
    "\n",
    "    return svc_lin,svc_rbf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confusion matrix and accuracy for all of the models of the test data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "model_names = ['SVC Linear', 'SVC RBF']\n",
    "\n",
    "for i in range(len(model)):\n",
    "    # 生成混淆矩陣\n",
    "    y_pred = model[i].predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # 計算準確率\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    test_score = (TP + TN) / (TN + TP + FP + FN)\n",
    "    \n",
    "    # 繪製混淆矩陣\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'{model_names[i]} Confusion Matrix\\nAccuracy = {test_score:.2f}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    # 顯示分類報告\n",
    "    print(f'{model_names[i]} Classification Report')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "對測試集資料進行相同的前處理，並使用訓練好的模型進行預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用與訓練集相同的方式處理測試集\n",
    "\n",
    "# 檢視缺失值\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# 移除欄位\n",
    "test_df = test_df.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# 填補缺失值\n",
    "test_df['Age'] = test_df['Age'].fillna(test_df['Age'].mean())\n",
    "test_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].mean())\n",
    "\n",
    "# 填補完成\n",
    "print('-'*30)\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "\n",
    "test_df.iloc[:, 1] = labelencoder.fit_transform(test_df.iloc[:, 1].values) # Sex\n",
    "test_df.iloc[:, 6] = labelencoder.fit_transform(test_df.iloc[:, 6].values) # Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into independent 'X' and dependent 'y' variables\n",
    "test_y_df = pd.read_csv('/Users/hank/CodeSpace/1131-ML/Assignment_2/dataset/Taitanic/gender_submission.csv')\n",
    "\n",
    "X_test = test_df\n",
    "y_test = test_y_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data \n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢視結果\n",
    "for i in range(len(model)):\n",
    "    # 生成混淆矩陣\n",
    "    y_pred = model[i].predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # 計算準確率\n",
    "    test_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # 繪製混淆矩陣\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'{model_names[i]} Confusion Matrix\\nAccuracy = {test_score:.2f}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # 顯示分類報告\n",
    "    print(f'{model_names[i]} Classification Report')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
